---
title: "p8105_hw5_zlb2008"
author: "Zakari L. Billo"
date: "2025-11-09"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 7,
  fig.align = "center",
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
set.seed(1)

bday_match <- function(n) {
  
  birthdays <- sample(1:365, size =  n, replace = TRUE)
  
  return(any(duplicated(birthdays)))
}
```

```{r}
n_sims <- 10000
group_sizes <- 2:50

probabilities <- sapply(group_sizes, function(n) {
  
    sim_results <- replicate(n_sims, bday_match(n))
    
    mean(sim_results)
    
})

results_df <- data.frame(
  n = group_sizes,
  probability = probabilities
  ) 

results_df |>
  ggplot(aes(x = n, y = probability)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  labs(
    title = "Birthday Problem Simulation",
    x = "Group Size (n)",
    y = "Probability of at least one shared birthday"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal() |>
  print()
```

The probability of a shared birhtday appears to be a positive monotinic function of group size. At a group sizes 23 and 50 there is a 51.30% and 97.03% chance of at least one shared birthday, respectively. The growth is rapid because we aren't checking for a match with a specific birthday or birthdate. Instead, we are checking for any match between any two people in the group.

## Problem 2

```{r}
set.seed(1)
n = 30
mus_to_test = 0:6 
sigma = 5
n_sims = 5000
alpha = 0.05

sim_results <- crossing(
  true_mu = mus_to_test,
  sim_id = 1:n_sims
  ) |>
  mutate(test_result = map(true_mu, ~ {
    
    sim_data_vector = rnorm(n, mean = .x, sd = sigma)
    
    broom::tidy(t.test(sim_data_vector, mu = 0))
  })) |>
  unnest(test_result)

sim_results |>
  select(
    true_mu,
    mu_hat = estimate, 
    p_value = p.value
  ) |>
  knitr::kable(digits = 3) |>
  head(10)
```

```{r}
sim_results |>
  group_by(true_mu) |>
  summarize(
    proportion_rejected = mean(p.value < alpha),
    .groups = "drop" 
  ) |> 
  ggplot(aes(x = true_mu, y = proportion_rejected)) +
  geom_line(color = "blue", linewidth =  1) +
  geom_point(color = "blue", linewidth =  3) +
  geom_hline(yintercept = alpha, linetype = "dashed", color = "red") +
  annotate("text", x = 0.5, y = 0.07, label = "Type I Error Rate (α = 0.05)",
           hjust = 0, color = "red") +
  labs(
    title = "Power & Type I Error by True Value of μ (n=30, σ=5)",
    subtitle = "H0: μ = 0  |  5,000 simulations per point",
    x = "True Value of μ",
    y = "Proportion of times H0 was rejected"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()
```

As effect size increases, power also increases. The fact that $H_0$ was rejected approximately 5% of times when the true value of $\mu$ is 0 confirms that $alpha$ is appropriately limiting the type 1 errors at the 0.05 threshhold. These results indicate the t test is working correctly for the simulation. 

```{r}
sim_results |>
  group_by(true_mu) |>
  summarize(
    `All Samples` = mean(estimate),
    `Rejected Samples Only` = mean(estimate[p.value < alpha]),
    .groups = "drop"
  ) |>
  pivot_longer(
    cols = c(`All Samples`, `Rejected Samples Only`),
    names_to = "group",
    values_to = "avg_mu_hat"
  ) |>
  ggplot(aes(x = true_mu, y = avg_mu_hat, color = group)) +
  geom_line(aes(linetype = group), linewidth =  1) +
  geom_point(aes(shape = group), linewidth =  3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", color = "black") +
  annotate("text", x = 1.3, y = 1.1, label = "y = x (Identity Line)", 
           hjust = 0, vjust = 0, linewidth =  3, color = "black") +
  labs(
    title = "Average μ Estimate vs. True μ",
    subtitle = "Comparing all samples vs. only 'significant' (p < 0.05) samples",
    x = "True Value of μ",
    y = "Average μ Estimat)",
    color = "Sample Group",
    linetype = "Sample Group",
    shape = "Sample Group"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("All Samples" = "blue", "Rejected Samples Only" = "red"))
```

The sample average of $\hat{\mu}$ across tests for which the null is rejected is not approximately equal to the true value of $\mu$, particularly between $0 < \mu < 3$. This is because low effect sizes worsen power, and produces selection bias. Filtering for $p < 0.05$ effectively captures overestimates, where the sample of 30 people produces a sample mean $\hat{\mu}$ that is unusually larger than the true mean. However, as $mu$ gets larger, the power approaches 100%, and the average $\hat{\mu}$ for the "rejected" group converges with the true $\mu$ and the average for the "All Samples" group.

## Problem 3 

```{r}
homicide_df <- read.csv('data/homicide-data.csv', na = c("NA", ".", "")) |>
  janitor::clean_names() 

summary(homicide_df)
```

The raw data has 52179 observations, corresponding to homicides, across 12 variables: `uid`, `reported_date`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`, `lon`, `disposition`. There are 120 NAs split evenly between `lat` and `lon`. 

```{r}
homicide_df <- homicide_df |>
  unite(
    col = "city_state", 
    city, state,  
    sep = ", ",    
    remove = TRUE   
  ) 

homicide_df |> 
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_closed_without = sum(disposition == "Closed without arrest"),
    unsolved_open = sum(disposition == "Open/No arrest"),
    .groups = "drop" 
  ) |>
  knitr::kable(
    caption = "Homicide Counts by City and Disposition",
    col.names = c("City-State", "Total Homicides", 
                  "Closed without Arrest", "Open/No Arrest")
  )
```

```{r}
homicide_df |>
  filter(city_state == "Baltimore, MD") |>
  mutate(
    unsolved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 1, 0)
  ) |>
  summarize(
    unsolved_count = sum(unsolved, na.rm = TRUE),
    total_count = n()  
  ) |>
  reframe(
    broom::tidy(prop.test(unsolved_count, total_count))
  ) |>
  select(estimate, conf.low, conf.high)
```
 
```{r}
get_city_prop_tests <- function(data) {
  data |>
    mutate(
      unsolved = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 1, 0)
    ) |>
    group_by(city_state) %>%
    summarize(
      unsolved_count = sum(unsolved, na.rm = TRUE),
      total_count = n(),
      .groups = "drop"
    ) |>
    mutate(test = map2(unsolved_count, total_count, ~ prop.test(.x, .y)),
           tidy = map(test, broom::tidy)) %>%
    unnest(tidy) |>
    select(city_state, estimate, conf.low, conf.high)
}

city_prop_tests <- get_city_prop_tests(homicide_df)
print(city_prop_tests)
```

```{r}
city_prop_tests |>
  mutate(city_state = reorder(city_state, estimate)
  ) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_col(fill = "lightpink") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.3) +
  coord_flip() +
  labs(
    x = "City",
    y = "Prop. Unsolved Homocides",
    title = "Proportion of Unsolved Homicides by City (95% CI)",
  ) +
  theme_minimal(base_size = 14)
```
